{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 38 Val: 10\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "`CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pps21\\Documents\\Cornell\\HeartTransplant\\train.py:125\u001b[0m\n\u001b[0;32m    121\u001b[0m     trainer\u001b[39m.\u001b[39mfit(model, datamodule\u001b[39m=\u001b[39mdm, ckpt_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[0;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 125\u001b[0m     main()\n",
      "File \u001b[1;32mc:\\Users\\pps21\\Documents\\Cornell\\HeartTransplant\\train.py:109\u001b[0m\n\u001b[0;32m    105\u001b[0m callbacks \u001b[39m=\u001b[39m [checkpoint_callback, lr_monitor, early_stop_callback]\n\u001b[0;32m    107\u001b[0m tb_logger \u001b[39m=\u001b[39m TensorBoardLogger(config[\u001b[39m'\u001b[39m\u001b[39mlogging\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mroot_path\u001b[39m\u001b[39m'\u001b[39m], config[\u001b[39m'\u001b[39m\u001b[39mlogging\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m], version\u001b[39m=\u001b[39mexperiment_name)\n\u001b[1;32m--> 109\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(\n\u001b[0;32m    110\u001b[0m     \u001b[39m# gpus=config.get('gpus'),\u001b[39;49;00m\n\u001b[0;32m    111\u001b[0m     max_epochs\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mepochs\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    112\u001b[0m     accelerator\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m# \"cuda\"\u001b[39;49;00m\n\u001b[0;32m    113\u001b[0m     gradient_clip_val\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mgrad_clip\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m0\u001b[39;49m),\n\u001b[0;32m    114\u001b[0m     log_every_n_steps\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mlogging\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtrain_logs_steps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    115\u001b[0m     num_sanity_val_steps\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m    116\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    117\u001b[0m     logger\u001b[39m=\u001b[39;49m[tb_logger],\n\u001b[0;32m    118\u001b[0m     precision\u001b[39m=\u001b[39;49mconfig[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mget(\u001b[39m'\u001b[39;49m\u001b[39mprecision\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m32\u001b[39;49m),\n\u001b[0;32m    119\u001b[0m )\n\u001b[0;32m    121\u001b[0m trainer\u001b[39m.\u001b[39mfit(model, datamodule\u001b[39m=\u001b[39mdm, ckpt_path\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mcheckpoint\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\pps21\\Anaconda3\\envs\\nlst_env\\lib\\site-packages\\pytorch_lightning\\utilities\\argparse.py:69\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     66\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[0;32m     68\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\pps21\\Anaconda3\\envs\\nlst_env\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:398\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[1;34m(self, accelerator, strategy, devices, num_nodes, precision, logger, callbacks, fast_dev_run, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, overfit_batches, val_check_interval, check_val_every_n_epoch, num_sanity_val_steps, log_every_n_steps, enable_checkpointing, enable_progress_bar, enable_model_summary, accumulate_grad_batches, gradient_clip_val, gradient_clip_algorithm, deterministic, benchmark, inference_mode, use_distributed_sampler, profiler, detect_anomaly, barebones, plugins, sync_batchnorm, reload_dataloaders_every_n_epochs, default_root_dir)\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m _DataConnector(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 398\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m _AcceleratorConnector(\n\u001b[0;32m    399\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[0;32m    400\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[0;32m    401\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m    402\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[0;32m    403\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[0;32m    404\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[0;32m    405\u001b[0m     use_distributed_sampler\u001b[39m=\u001b[39;49muse_distributed_sampler,\n\u001b[0;32m    406\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[0;32m    407\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[0;32m    408\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[0;32m    409\u001b[0m )\n\u001b[0;32m    410\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m _LoggerConnector(\u001b[39mself\u001b[39m)\n\u001b[0;32m    411\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m _CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\pps21\\Anaconda3\\envs\\nlst_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:157\u001b[0m, in \u001b[0;36m_AcceleratorConnector.__init__\u001b[1;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, sync_batchnorm, benchmark, use_distributed_sampler, deterministic)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_gpu_accelerator_backend()\n\u001b[0;32m    156\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_device_config_and_set_final_flags(devices\u001b[39m=\u001b[39mdevices, num_nodes\u001b[39m=\u001b[39mnum_nodes)\n\u001b[1;32m--> 157\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_parallel_devices_and_init_accelerator()\n\u001b[0;32m    159\u001b[0m \u001b[39m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[0;32m    160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[1;32mc:\\Users\\pps21\\Anaconda3\\envs\\nlst_env\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\accelerator_connector.py:381\u001b[0m, in \u001b[0;36m_AcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m accelerator_cls\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m    376\u001b[0m     available_accelerator \u001b[39m=\u001b[39m [\n\u001b[0;32m    377\u001b[0m         acc_str\n\u001b[0;32m    378\u001b[0m         \u001b[39mfor\u001b[39;00m acc_str \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types\n\u001b[0;32m    379\u001b[0m         \u001b[39mif\u001b[39;00m AcceleratorRegistry[acc_str][\u001b[39m\"\u001b[39m\u001b[39maccelerator\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mis_available()\n\u001b[0;32m    380\u001b[0m     ]\n\u001b[1;32m--> 381\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[0;32m    382\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`\u001b[39m\u001b[39m{\u001b[39;00maccelerator_cls\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m` can not run on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    385\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer`: \u001b[39m\u001b[39m{\u001b[39;00mavailable_accelerator\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m     )\n\u001b[0;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[0;32m    389\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_devices_flag \u001b[39m=\u001b[39m accelerator_cls\u001b[39m.\u001b[39mparse_devices(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_devices_flag)\n",
      "\u001b[1;31mMisconfigurationException\u001b[0m: `CUDAAccelerator` can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "from omegaconf import OmegaConf\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.lr_monitor import LearningRateMonitor\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from data.dataloader import NLSTDataModule\n",
    "from models import NLSTTrainingModule\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Processing configuration for training')\n",
    "    parser.add_argument('--config', type=str, help='path to config file', default='configs/config.yaml')\n",
    "    # args = parser.parse_args()\n",
    "    args, unknown = parser.parse_known_args() # solution for jupyter run\n",
    "\n",
    "    pl.seed_everything(int(os.environ.get('LOCAL_RANK', 0)))\n",
    "\n",
    "    # Load configuration file\n",
    "    config = OmegaConf.load(args.config)\n",
    "\n",
    "    raw_directory = config['data']['raw_directory']\n",
    "    annotations_dir = config['data']['annotations_dir']\n",
    "\n",
    "    data_dict = {}\n",
    "    for raw_sample in glob.glob(raw_directory + '**'):\n",
    "        # sample_id = raw_sample.split('/')[-1][:-4]\n",
    "        if '.nii' in raw_sample:\n",
    "            sample_id = raw_sample.split('\\\\')[-1][:-4]\n",
    "            for label_path in glob.glob(annotations_dir + '**'):\n",
    "                if sample_id in label_path and '.nii' in label_path and all(sample_id not in key for key in data_dict.keys()):\n",
    "                    data_dict.update({raw_sample:label_path})\n",
    "\n",
    "    # Data split\n",
    "    splits = pd.read_csv(config['data']['data_splits'])\n",
    "\n",
    "    data_dict_train, data_dict_val = {}, {}\n",
    "    for index, row in splits.iterrows():\n",
    "        patient_id = row['id']\n",
    "        split = row['split'] #set\n",
    "\n",
    "        for key in data_dict.keys():\n",
    "            if str(patient_id) in key and split == 'train':\n",
    "                data_dict_train.update({key: data_dict[key]})\n",
    "            if str(patient_id) in key and split == 'val' and str(patient_id) != '100092':\n",
    "                data_dict_val.update({key: data_dict[key]})\n",
    "\n",
    "    print('Train:', len(data_dict_train), 'Val:', len(data_dict_val))\n",
    "\n",
    "    # Init Lightning Data Module\n",
    "    dm = NLSTDataModule(\n",
    "        data_dict_train=data_dict_train,\n",
    "        data_dict_val=data_dict_val,\n",
    "        nii_format=True,\n",
    "        batch_size=config['data']['batch_size_per_gpu'],\n",
    "        num_workers=config['data']['dataloader_workers_per_gpu'],\n",
    "        target_size=config['data']['target_size'],\n",
    "        transform=config['train'].get('aug'),\n",
    "        crop_heart=config['data'].get('crop_heart')\n",
    "    )\n",
    "\n",
    "    # Init model\n",
    "    model = NLSTTrainingModule(\n",
    "        net=config['model']['name'],\n",
    "        lr=config['train']['lr'],\n",
    "        loss=config['train']['loss'],\n",
    "        pretrained_weights=config['model'].get('pretrained_weights')\n",
    "    )\n",
    "\n",
    "    # Set callbacks\n",
    "    if os.environ.get('LOCAL_RANK', 0) == 0:\n",
    "        experiment_name = '{}__{}'.format(\n",
    "            config['model']['name'],\n",
    "            str(datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\"))\n",
    "        )\n",
    "    else:\n",
    "        experiment_name, log_path = '', ''\n",
    "\n",
    "    # create logging directory\n",
    "    log_path = os.path.join(config['logging']['root_path'], config['logging']['name'], experiment_name)\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path, exist_ok=True)\n",
    "    # write main config\n",
    "    with open(os.path.join(log_path, 'config.yaml'), 'w') as f:\n",
    "        f.write(OmegaConf.to_yaml(config))\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=log_path,\n",
    "        filename='{step:d}',\n",
    "        every_n_epochs=1,\n",
    "        save_top_k=3,\n",
    "        mode='min',\n",
    "        monitor='val_epoch/loss',\n",
    "        auto_insert_metric_name=True\n",
    "    )\n",
    "    lr_monitor = LearningRateMonitor()\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_epoch/loss\", min_delta=0.0, patience=10, verbose=False, mode=\"min\")\n",
    "    callbacks = [checkpoint_callback, lr_monitor, early_stop_callback]\n",
    "\n",
    "    tb_logger = TensorBoardLogger(config['logging']['root_path'], config['logging']['name'], version=experiment_name)\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        # gpus=config.get('gpus'),\n",
    "        max_epochs=config['train']['epochs'],\n",
    "        accelerator=\"cuda\", # \"cuda\"\n",
    "        gradient_clip_val=config['train'].get('grad_clip', 0),\n",
    "        log_every_n_steps=config['logging']['train_logs_steps'],\n",
    "        num_sanity_val_steps=0,\n",
    "        callbacks=callbacks,\n",
    "        logger=[tb_logger],\n",
    "        precision=config['train'].get('precision', 32),\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, datamodule=dm, ckpt_path=config.get('checkpoint'))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
